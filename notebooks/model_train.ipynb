{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e632c1a4-71d0-4f2a-8058-363ecb2dae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Conformer as con\n",
    "from data_processing import ukr_lang_chars_handle\n",
    "from data_processing import CommonVoiceUkr\n",
    "from model.conformer import Conformer as con\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230cc976-c2c4-4f64-8680-8dd486f717a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "input lengths: tensor([256, 256, 256, 256, 256, 256, 256, 256])\n",
      "torch.Size([8])\n",
      "target lengths: tensor([152, 152, 152, 152, 152, 152, 152, 152])\n"
     ]
    }
   ],
   "source": [
    "tgt_n = 152\n",
    "target = torch.randn(BATCH_SIZE, tgt_n) # (N, S) where N =batch size and S = max target length \n",
    "\n",
    "outputs = torch.randn(BATCH_SIZE, 1, 256, 38) # Tensor of size (T, N, C), where T = input length, N = batch size, and C = number of classes (including blank)\n",
    "b, cnls, t, clss = outputs.shape\n",
    "outputs = outputs.view(t*cnls, b, clss)\n",
    "\n",
    "\n",
    "input_lengths = torch.full(size=(BATCH_SIZE,), fill_value=outputs.shape[0], dtype=torch.long)\n",
    "target_lengths = torch.full(size=(BATCH_SIZE,), fill_value=target.shape[-1], dtype=torch.long)\n",
    "print(input_lengths.shape)\n",
    "print(\"input lengths:\", input_lengths)\n",
    "\n",
    "print(target_lengths.shape)\n",
    "print(\"target lengths:\", target_lengths)\n",
    "\n",
    "ctc_loss = nn.CTCLoss(zero_infinity=False, reduction=\"none\")\n",
    "loss = ctc_loss(outputs, target, input_lengths, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c02c55-2237-4a30-ac5e-8fcf9f5d3245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(BATCH_SIZE)\n",
    "ds = CommonVoiceUkr(TRAIN_PATH, TRAIN_SPEC_PATH)\n",
    "n_ds = len(ds)\n",
    "n_ds = n_ds - n_ds % BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ad63d5-44fa-4589-8cb8-85cfaba74ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch â„–1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [01:47,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 9.1475, Loss mean: 9.3085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [03:35,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 7.1415, Loss mean: 9.0365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [05:22,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 4.6850, Loss mean: 8.2542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [06:41,  2.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2748/3731362048.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hots\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch, _, time, n_class)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcnls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclss\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (time, batch, n_class)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\conformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, tgt)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_enc_inp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_enc_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\conformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mPE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\conformer.py\u001b[0m in \u001b[0;36mposition_encoding\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                     \u001b[0mpe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mpe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\conformer.py\u001b[0m in \u001b[0;36mpos_f\u001b[1;34m(self, row, column, emb_dim)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mw_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpe_i_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpe_i_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mposition_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Conformer as con\n",
    "from data_processing import ukr_lang_chars_handle\n",
    "from data_processing import CommonVoiceUkr\n",
    "from config import *\n",
    "from torch.optim import RAdam\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import numpy as np\n",
    "import wandb\n",
    "#wandb.init(project=\"ASR\", entity=\"alex2135\")\n",
    "\n",
    "#wandb.config = CONFIG\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Making dataset and loader\n",
    "ds = CommonVoiceUkr(TRAIN_PATH, TRAIN_SPEC_PATH, batch_size=BATCH_SIZE)\n",
    "train_dataloader = DataLoader(ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "tgt_n = 152\n",
    "model = con(n_encoders=8, n_decoders=8, device=device)\n",
    "\n",
    "# Create optimizator\n",
    "optimizer = RAdam(model.parameters(),lr=CONFIG[\"learning_rate\"])\n",
    "\n",
    "\n",
    "# Create CTC criterion\n",
    "criterion = nn.CTCLoss(blank=ukr_lang_chars_handle.token_to_index[\"<blank>\"], zero_infinity=True)\n",
    "\n",
    "\n",
    "running_loss = []\n",
    "epochs = CONFIG[\"epochs\"]\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch â„–{epoch}\")\n",
    "    for idx, (X, tgt) in tqdm(enumerate(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt, 152).to(device)\n",
    "        X = X.to(device)\n",
    "        \n",
    "        output = model(X, one_hots)  # (batch, _, time, n_class)\n",
    "        b, cnls, t, clss = output.shape\n",
    "        output = output.view(t * cnls, b, clss)  # (time, batch, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        indeces = ukr_lang_chars_handle.sentences_to_indeces(tgt).to(device)\n",
    "        \n",
    "        input_lengths = torch.full(size=(BATCH_SIZE,), fill_value=t, dtype=torch.long).to(device)\n",
    "        target_lengths = torch.full(size=(BATCH_SIZE,), fill_value=indeces.shape[-1], dtype=torch.long).to(device)\n",
    "        loss = criterion(output, indeces.cpu(), input_lengths.cpu(), target_lengths.cpu())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss)\n",
    "        #wandb.log({\"loss\": loss})\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(\"Target label:\", tgt)\n",
    "            print(\"Running loss:\")\n",
    "            pprint.pprint(running_loss)\n",
    "            print(output.shape)\n",
    "            print(\"Is nan in output:\", torch.sum(torch.isnan(output)))\n",
    "            print(\"Is inf in output:\", torch.sum(torch.isinf(output)))\n",
    "            pprint.pprint(output)\n",
    "            break\n",
    "        if (idx + 1) % 50 == 0:  # print every 200 mini-batches\n",
    "            running_loss = [t.cpu().detach().numpy() if type(t) is torch.Tensor else t for t in running_loss]\n",
    "            running_loss = np.array(running_loss)\n",
    "            print(f\"Epoch: {epoch}, Last loss: {loss:.4f}, Loss mean: {np.mean(running_loss):.4f}\")\n",
    "            running_loss = list(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826bb23d-afe3-48ef-8721-58b81b20ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses_list = [t.cpu().detach().numpy() if type(t) is torch.Tensor else t for t in running_loss ]\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(losses_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e7aa3-53f5-44c2-baac-7bcb63a4f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wtout_zeros = np.array([t.cpu().detach().numpy() if type(t) is torch.Tensor else t for t in running_loss])\n",
    "print(len(running_loss))\n",
    "wtout_zeros = wtout_zeros[wtout_zeros != 0]\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(wtout_zeros)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bc36b-e045-44ee-ad64-f1d3fa236967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "model = con(device=device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f735e-fe53-4be6-b905-a2626df9ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeff538-a4b8-448a-8aa0-080043beb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = (\"ÐŸÑ€Ð¸Ð²Ñ–Ñ‚\",)\n",
    "oh_sent = ukr_lang_chars_handle.sentences_to_one_hots(tgt, 152)\n",
    "#print(oh_sent)\n",
    "\n",
    "result = ukr_lang_chars_handle.one_hots_to_sentence(oh_sent)\n",
    "#print(result)\n",
    "indeces = ukr_lang_chars_handle.sentence_to_indeces(sent[0])\n",
    "#print(indeces)\n",
    "\n",
    "one_hots = F.one_hot(torch.Tensor(indeces).long(), num_classes=38)\n",
    "#print(one_hots)\n",
    "\n",
    "\n",
    "reproduced_sent = ukr_lang_chars_handle.onehot_matrix_to_idxs(one_hots)\n",
    "#print(f\"{reproduced_sent=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe5b77-258f-4f60-93e3-0e4e443cb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_processing import ukr_lang_chars_handle\n",
    "from config import *\n",
    "from model import Conformer as con\n",
    "from data_processing import CommonVoiceUkr\n",
    "from torch.utils.data import DataLoader\n",
    "import pprint\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "model = con(device=device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "model.eval()\n",
    "ds = CommonVoiceUkr(TRAIN_PATH, TRAIN_SPEC_PATH)\n",
    "train_dataloader = DataLoader(ds, shuffle=True, batch_size=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, tgt = next(iter(train_dataloader))\n",
    "    X = X.to(device)\n",
    "    print(\"Target:\", tgt)\n",
    "    print(\"X shape:\", X.shape)\n",
    "    #tgt = (\"\",)\n",
    "\n",
    "    tgt_one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt, 152)\n",
    "    print(\"tgt to one_hots shape:\", tgt_one_hots.shape)\n",
    "    print(\"tgt to one_hots:\", ukr_lang_chars_handle.one_hots_to_sentences(tgt_one_hots))\n",
    "\n",
    "    out_data = model(X, tgt_one_hots.to(device))\n",
    "    out_data = F.softmax(out_data, dim=-1)\n",
    "    out_data = out_data.cpu()\n",
    "    print(\"\\n\\nOutput data shape:\", out_data.shape)\n",
    "    print(\"output:\", out_data)\n",
    "    out_data = out_data.transpose(-1, -2).contiguous()\n",
    "    result = ukr_lang_chars_handle.one_hots_to_sentences(out_data)\n",
    "    pprint.pprint(len(result))\n",
    "    pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35493a22-eba4-4223-8119-c3a153af1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "print(PATH)\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78cf4bc-8d33-4ce6-af74-24cf5c65875b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

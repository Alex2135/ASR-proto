{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e632c1a4-71d0-4f2a-8058-363ecb2dae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Conformer as con\n",
    "from data_processing import ukr_lang_chars_handle\n",
    "from data_processing import CommonVoiceUkr\n",
    "from model.conformer import Conformer as con\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "230cc976-c2c4-4f64-8680-8dd486f717a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "input lengths: tensor([256, 256])\n",
      "torch.Size([2])\n",
      "target lengths: tensor([152, 152])\n"
     ]
    }
   ],
   "source": [
    "tgt_n = 152\n",
    "target = torch.randn(BATCH_SIZE, tgt_n) # (N, S) where N =batch size and S = max target length \n",
    "\n",
    "outputs = torch.randn(BATCH_SIZE, 1, 256, 38) # Tensor of size (T, N, C), where T = input length, N = batch size, and C = number of classes (including blank)\n",
    "b, cnls, t, clss = outputs.shape\n",
    "outputs = outputs.view(t*cnls, b, clss)\n",
    "\n",
    "\n",
    "input_lengths = torch.full(size=(BATCH_SIZE,), fill_value=outputs.shape[0], dtype=torch.long)\n",
    "target_lengths = torch.full(size=(BATCH_SIZE,), fill_value=target.shape[-1], dtype=torch.long)\n",
    "print(input_lengths.shape)\n",
    "print(\"input lengths:\", input_lengths)\n",
    "\n",
    "print(target_lengths.shape)\n",
    "print(\"target lengths:\", target_lengths)\n",
    "\n",
    "ctc_loss = nn.CTCLoss(zero_infinity=False, reduction=\"none\")\n",
    "loss = ctc_loss(outputs, target, input_lengths, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c02c55-2237-4a30-ac5e-8fcf9f5d3245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(BATCH_SIZE)\n",
    "ds = CommonVoiceUkr(TRAIN_PATH, TRAIN_SPEC_PATH)\n",
    "n_ds = len(ds)\n",
    "n_ds = n_ds - n_ds % BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ad63d5-44fa-4589-8cb8-85cfaba74ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch №1\n",
      "Training begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:14,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 1.9048, Loss phase mean: 1.6208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:26,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 1.6225, Loss phase mean: 1.6693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [00:30,  1.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24768/1956151937.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24768/1956151937.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch №{epoch}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[0mval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_val_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24768/1956151937.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, optimizer, device, scheduler, epoch, wb)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hots\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch, _, n_class, time), (batch, _, time, n_class)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mce_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# output.shape == (N, C) where N - batch, C - number of classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\conformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, tgt)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommandClassifierByEncoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m         \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\conformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, tgt)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_enc_inp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_enc_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\conformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mPE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\conformer.py\u001b[0m in \u001b[0;36mposition_encoding\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                     \u001b[0mpe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mpe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\conformer.py\u001b[0m in \u001b[0;36mpos_f\u001b[1;34m(self, row, column, emb_dim)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mw_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpe_i_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpe_i_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mposition_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "\n",
    "from config import *\n",
    "from data_processing import ukr_lang_chars_handle\n",
    "from data_processing import CommonVoiceUkr\n",
    "from model import CommandClassifierByEncoder as Model\n",
    "from model import get_cosine_schedule_with_warmup, OneCycleLR\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, optimizer, device, scheduler=None, epoch=1, wb=None):\n",
    "    print(f\"Training begin\")\n",
    "    model.train()\n",
    "    ce_criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = []\n",
    "    losses_per_phase = []\n",
    "\n",
    "    for idx, (X, tgt) in tqdm(enumerate(train_dataloader)):\n",
    "        tgt_text = \" \"#tgt[\"text\"]\n",
    "        tgt_class = tgt[\"label\"].long().to(device)\n",
    "        tgt_class = F.one_hot(tgt_class, num_classes=5)\n",
    "\n",
    "        one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt_text, 152).to(device)\n",
    "        X = X.to(device) #\n",
    "\n",
    "        emb, output = model(X, one_hots)  # (batch, _, n_class, time), (batch, _, time, n_class)\n",
    "        loss = ce_criterion(output, tgt_class.float()) # output.shape == (N, C) where N - batch, C - number of classes\n",
    "        if wb:\n",
    "            wb.log({\n",
    "                \"loss\": loss.item(),\n",
    "                \"epoch\": epoch\n",
    "            })\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "        losses_per_phase.append(loss.cpu().detach().numpy())\n",
    "        if (idx + 1) % 25 == 0:  # print every 200 mini-batches\n",
    "            loss_mean = np.mean(np.array(losses_per_phase))\n",
    "            print(f\"Epoch: {epoch}, Last loss: {loss.item():.4f}, Loss phase mean: {loss_mean:.4f}\")\n",
    "            if wb:\n",
    "                wb.log({\"loss phase mean\": loss_mean})\n",
    "            losses_per_phase = []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def val(model, train_dataloader, device, epoch, wb=None):\n",
    "    model.eval()\n",
    "    positive = 0\n",
    "    train_len = train_dataloader.sampler.num_samples\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Evaluation on train dataset\")\n",
    "    with torch.no_grad():\n",
    "        for idx, (X, tgt) in tqdm(enumerate(train_dataloader)):\n",
    "            tgt_text = \" \"#tgt[\"text\"]\n",
    "            tgt_class = tgt[\"label\"].long().to(device)\n",
    "            tgt_class = F.one_hot(tgt_class, num_classes=5)\n",
    "            one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt_text, 152).to(device)\n",
    "            X = X.to(device)\n",
    "            emb, output = model(X, one_hots)\n",
    "            A = torch.argmax(output, dim=-1)\n",
    "            B = torch.argmax(tgt_class, dim=-1)\n",
    "            is_right = (A == B)\n",
    "            positive += torch.sum(is_right)\n",
    "\n",
    "    train_accuracy = positive / train_len\n",
    "    if wb:\n",
    "        wb.log({\n",
    "            \"train accuracy\": train_accuracy,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "    print(f\"Accuracy on TRAIN dataset: {train_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "def get_scheduler(epochs, train_len, optimizer, scheduler_name=\"cosine_with_warmup\"):\n",
    "    #wandb.config[\"scheduler\"] = scheduler_name\n",
    "    if scheduler_name == \"cosine_with_warmup\":\n",
    "        return get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=epochs//5,\n",
    "                                                num_training_steps=epochs - epochs//5)\n",
    "    elif scheduler_name == \"constant\":\n",
    "        return torch.optim.lr_scheduler.ConstantLR(optimizer)\n",
    "    elif scheduler_name == \"exponential\":\n",
    "        return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "    elif scheduler_name == \"one_circle\":\n",
    "        return OneCycleLR(optimizer,\n",
    "                          max_lr=CONFIG[\"learning_rate\"]*10,\n",
    "                          total_steps=train_len)\n",
    "\n",
    "\n",
    "def main():\n",
    "    #wandb_stat = wandb.init(project=\"ASR\", entity=\"Alex2135\", config=CONFIG)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Making dataset and loader\n",
    "    ds = CommonVoiceUkr(TRAIN_PATH, TRAIN_SPEC_PATH, batch_size=BATCH_SIZE)\n",
    "    train_dataloader = DataLoader(ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    train_val_dataloader = DataLoader(ds, shuffle=True, batch_size=64)\n",
    "    epochs = CONFIG[\"epochs\"]\n",
    "    train_len = len(train_dataloader) * epochs\n",
    "\n",
    "    tgt_n = 152\n",
    "    model = Model(n_encoders=4, n_decoders=CONFIG[\"n_decoders\"], device=device)\n",
    "    if CONFIG[\"pretrain\"] == True:\n",
    "        PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "        model = Model(n_encoders=CONFIG[\"n_encoders\"], n_decoders=CONFIG[\"n_decoders\"], device=device)\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    # Create optimizator\n",
    "    optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "    save_model = True\n",
    "    scheduler = get_scheduler(CONFIG[\"epochs\"], train_len, optimizer, \"cosine_with_warmup\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch №{epoch}\")\n",
    "        train(model, train_dataloader, optimizer, device, scheduler=scheduler, epoch=epoch)\n",
    "        val(model, train_val_dataloader, device, epoch)\n",
    "        scheduler.step(epoch)\n",
    "        #wandb.log({\"scheduler lr\": scheduler.get_last_lr()})\n",
    "\n",
    "    if save_model:\n",
    "        PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "        print(f\"Save model to path: '{PATH}'\")\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826bb23d-afe3-48ef-8721-58b81b20ad26",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'running_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24768/650795614.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlosses_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'running_loss' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses_list = [t.cpu().detach().numpy() if type(t) is torch.Tensor else t for t in running_loss ]\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(losses_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e7aa3-53f5-44c2-baac-7bcb63a4f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wtout_zeros = np.array([t.cpu().detach().numpy() if type(t) is torch.Tensor else t for t in running_loss])\n",
    "print(len(running_loss))\n",
    "wtout_zeros = wtout_zeros[wtout_zeros != 0]\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(wtout_zeros)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bc36b-e045-44ee-ad64-f1d3fa236967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "model = con(device=device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f735e-fe53-4be6-b905-a2626df9ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeff538-a4b8-448a-8aa0-080043beb475",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = (\"Привіт\",)\n",
    "oh_sent = ukr_lang_chars_handle.sentences_to_one_hots(tgt, 152)\n",
    "#print(oh_sent)\n",
    "\n",
    "result = ukr_lang_chars_handle.one_hots_to_sentence(oh_sent)\n",
    "#print(result)\n",
    "indeces = ukr_lang_chars_handle.sentence_to_indeces(sent[0])\n",
    "#print(indeces)\n",
    "\n",
    "one_hots = F.one_hot(torch.Tensor(indeces).long(), num_classes=38)\n",
    "#print(one_hots)\n",
    "\n",
    "reproduced_sent = ukr_lang_chars_handle.onehot_matrix_to_idxs(one_hots)\n",
    "#print(f\"{reproduced_sent=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe5b77-258f-4f60-93e3-0e4e443cb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from data_processing import ukr_lang_chars_handle\n",
    "from config import *\n",
    "from model import Conformer as con\n",
    "from data_processing import CommonVoiceUkr\n",
    "from torch.utils.data import DataLoader\n",
    "import pprint\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "PATH = os.path.join(DATA_DIR, \"model_2.pt\")\n",
    "model = con(n_encoders=8, n_decoders=8, device=device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "model.eval()\n",
    "ds = CommonVoiceUkr(TRAIN_PATH, TRAIN_SPEC_PATH)\n",
    "train_dataloader = DataLoader(ds, shuffle=True, batch_size=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X, tgt = next(iter(train_dataloader))\n",
    "    X = X.to(device)\n",
    "    print(\"Target:\", tgt)\n",
    "    print(\"X shape:\", X.shape)\n",
    "    #tgt = (\"\",)\n",
    "\n",
    "    tgt_one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt, 152)\n",
    "    print(\"tgt to one_hots shape:\", tgt_one_hots.shape)\n",
    "    print(\"tgt to one_hots:\", ukr_lang_chars_handle.one_hots_to_sentences(tgt_one_hots))\n",
    "\n",
    "    out_data = model(X, tgt_one_hots.to(device))\n",
    "    out_data = F.softmax(out_data, dim=-1)\n",
    "    out_data = out_data.cpu()\n",
    "    print(\"\\n\\nOutput data shape:\", out_data.shape)\n",
    "    print(\"output:\", out_data)\n",
    "    out_data = out_data.transpose(-1, -2).contiguous()\n",
    "    result = ukr_lang_chars_handle.one_hots_to_sentences(out_data)\n",
    "    pprint.pprint(len(result))\n",
    "    pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35493a22-eba4-4223-8119-c3a153af1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "print(PATH)\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78cf4bc-8d33-4ce6-af74-24cf5c65875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 6\n",
    "\n",
    "A = torch.randn(2, 1, 64, 256)\n",
    "As = A.repeat(1, 6, 1, 1).transpose(-1, -2).contiguous()\n",
    "As.shape\n",
    "#torch.stack(A, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2584cb-df35-4743-bcdc-5458ed30ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(2, 1, 64, 256)\n",
    "\n",
    "norm = nn.LayerNorm(256)\n",
    "norm(A).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156dcbf-68df-4aa2-ac16-ba2299719ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MaskedSoftmaxCELoss\n",
    "\n",
    "\n",
    "X = torch.randn(8, 1, 64, 256)\n",
    "X = torch.squeeze(X, 1)\n",
    "lin1 = nn.Linear(256, 152)\n",
    "lin2 = nn.Linear(64, 38)\n",
    "X = lin1(X)\n",
    "X = lin2(X.transpose(-1, -2).contiguous())\n",
    "X = X.transpose(-1, -2).contiguous()\n",
    "print(X.shape)\n",
    "\n",
    "oh = torch.randn(8, 1, 38, 152)\n",
    "oh = eleminate_channels(oh)\n",
    "\n",
    "\n",
    "ce_criterion = MaskedSoftmaxCELoss()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss(X, oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067caf7-ab30-481e-b682-dff8bdbd3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin1 = nn.Sequential(nn.Linear(38*256, 2048), nn.ReLU())\n",
    "lin2 = nn.Sequential(nn.Linear(2048, 1024), nn.ReLU())\n",
    "lin3 = nn.Sequential(nn.Linear(1024, 5), nn.Softmax(dim=-1))\n",
    "\n",
    "X = torch.randn([256, 1, 38])\n",
    "t, b, d = X.shape\n",
    "X = X.view(b, t*d)\n",
    "\n",
    "X = lin1(X)\n",
    "X = lin2(X)\n",
    "X = lin3(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe15a8-2637-49fa-9b2f-186b9ff0ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "X = torch.Tensor([2, 0, 0, 1, 0, 3, 1, 1, 1, 2, 1, 2, 4, 1, 2, 1])\n",
    "X = torch.Tensor([X])\n",
    "X = F.one_hot(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c9506-8356-4ec9-9850-51c68624d35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

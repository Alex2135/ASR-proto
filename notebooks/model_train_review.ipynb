{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5684ef0-3bdc-42ae-820a-a6addd88c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfd29903-e0dd-49a6-977a-f52f4f9ffd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from config import *\n",
    "from data_processing import ukr_lang_chars_handle\n",
    "from data_processing import CommonVoiceUkr\n",
    "from model import EfConfRecognizer as Model\n",
    "from model import get_cosine_schedule_with_warmup, OneCycleLR\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, optimizer, device, scheduler=None, epoch=1, wb=None):\n",
    "    print(f\"Training begin\")\n",
    "    model.train()\n",
    "    ctc_criterion = nn.CTCLoss(reduction=\"none\", blank=ukr_lang_chars_handle.token_to_index[\"<blank>\"])#, zero_infinity=True)\n",
    "    running_loss = []\n",
    "    losses_per_phase = []\n",
    "    train_len = len(train_dataloader)\n",
    "\n",
    "    for idx, (X, tgt) in tqdm(enumerate(train_dataloader)):\n",
    "        tgt_text = tgt[\"text\"]\n",
    "        tgt_class = torch.Tensor(tgt[\"label\"]).long().to(device)\n",
    "        tgt_class = F.one_hot(tgt_class, num_classes=5)\n",
    "\n",
    "        tgt_lengths = [len(txt) for txt in tgt_text]\n",
    "        tgt_max_len = max(tgt_lengths)\n",
    "        one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt_text, tgt_max_len).to(device)\n",
    "        one_hots = one_hots.squeeze(dim=1).permute(0, 2, 1).float()\n",
    "        \n",
    "        X = X.to(device) #\n",
    "        X = X.squeeze(dim=1).permute(0, 2, 1)\n",
    "        #print(f\"{X.shape=}\")\n",
    "        #print(f\"{one_hots.shape=}\")\n",
    "\n",
    "        emb, output = model(X, one_hots)  # (batch, time, n_class), (batch, time, n_class)\n",
    "        \n",
    "        output = output.permute(1, 0, 2).to(device).detach().requires_grad_()\n",
    "        indeces = ukr_lang_chars_handle.sentences_to_indeces(tgt_text).to(device)\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"tgt_text:\")\n",
    "        pprint(tgt_text)\n",
    "        print(\"indeces:\")\n",
    "        pprint(indeces)\n",
    "        print(f\"Inputs shape: {output.shape}\")\n",
    "        print(f\"Tgt shape: {indeces.shape}\")\n",
    "        print(f\"one_hots shape: {one_hots.shape}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        input_lengths = torch.full(size=(output.shape[1],), fill_value=output.shape[-2], dtype=torch.long).to(device)\n",
    "        target_lengths = torch.full(size=(output.shape[1],), fill_value=tgt_max_len, dtype=torch.long).to(device)        \n",
    "        \n",
    "        loss = ctc_criterion(output.cpu(), indeces, input_lengths, target_lengths)\n",
    "        if wb:\n",
    "            wb.log({\n",
    "                \"loss\": loss.item(),\n",
    "                \"epoch\": epoch\n",
    "            })\n",
    "        #print(f\"{loss=}\")\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "        losses_per_phase.append(loss.cpu().detach().numpy())\n",
    "        if (idx + 1) % 10 == 0:  # print every 200 mini-batches\n",
    "            loss_mean = np.mean(np.array(losses_per_phase))\n",
    "            print(f\"Epoch: {epoch}, Last loss: {loss.item():.4f}, Loss phase mean: {loss_mean:.4f}\")\n",
    "            if wb:\n",
    "                wb.log({\"loss phase mean\": loss_mean})\n",
    "            losses_per_phase = []\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60bc1cbc-5bb4-4451-9ed1-158c3651a725",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7624/3849045339.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7624/3849045339.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# Making dataset and loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCommonVoiceUkr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_REC_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRAIN_REC_SPEC_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mtrain_val_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\data_processing\\dataset_handling.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, txt_path, img_dir, pad_dim1, pad_dim2, transform, batch_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspeech_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sentence\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"spectro_path\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'spectro_path'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_dim1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_dim1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_dim2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_dim2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3457\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3458\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3459\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3363\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def val(model, train_dataloader, device, epoch, wb=None):\n",
    "    model.eval()\n",
    "    positive = 0\n",
    "    train_len = train_dataloader.sampler.num_samples\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Evaluation on train dataset\")\n",
    "    with torch.no_grad():\n",
    "        for idx, (X, tgt) in tqdm(enumerate(train_dataloader)):\n",
    "            tgt_text = \" \"#tgt[\"text\"]\n",
    "            tgt_class = torch.Tensor(tgt[\"label\"]).long().to(device)\n",
    "            tgt_class = F.one_hot(tgt_class, num_classes=5)\n",
    "            one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt_text, 152).to(device)\n",
    "            one_hots = one_hots.squeeze(dim=1).permute(0, 2, 1).float()\n",
    "            X = X.to(device)  #\n",
    "            X = X.squeeze(dim=1).permute(0, 2, 1)\n",
    "            emb, output = model(X, one_hots)\n",
    "            A = torch.argmax(output, dim=-1)\n",
    "            B = torch.argmax(tgt_class, dim=-1)\n",
    "            is_right = (A == B)\n",
    "            positive += torch.sum(is_right)\n",
    "\n",
    "    train_accuracy = positive / train_len\n",
    "    if wb:\n",
    "        wb.log({\n",
    "            \"train accuracy\": train_accuracy,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "    print(f\"Accuracy on TRAIN dataset: {train_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "\n",
    "def get_scheduler(epochs, train_len, optimizer, scheduler_name=\"cosine_with_warmup\", wb=None):\n",
    "    if wb:\n",
    "        wb.config[\"scheduler\"] = scheduler_name\n",
    "    if scheduler_name == \"cosine_with_warmup\":\n",
    "        return get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=epochs//5,\n",
    "                                                num_training_steps=epochs - epochs//5,\n",
    "                                                num_cycles=0.5)#1.25)\n",
    "    elif scheduler_name == \"constant\":\n",
    "        return torch.optim.lr_scheduler.ConstantLR(optimizer)\n",
    "    elif scheduler_name == \"exponential\":\n",
    "        return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "    elif scheduler_name == \"one_circle\":\n",
    "        return OneCycleLR(optimizer,\n",
    "                          max_lr=CONFIG[\"learning_rate\"]*10,\n",
    "                          total_steps=train_len)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    Xs, LBLs = zip(*data)\n",
    "    Xs_out = pad_sequence([X.permute(0, 2, 1).squeeze(dim=0) for X in Xs], batch_first=True)\n",
    "    lbl1 = LBLs[0]\n",
    "    d_out = {}\n",
    "    for key in lbl1.keys():\n",
    "        d_out[key] = [d[key] for d in LBLs]\n",
    "    return Xs_out, d_out\n",
    "\n",
    "\n",
    "def main():\n",
    "    wandb_stat = None#wandb.init(project=\"ASR\", entity=\"Alex2135\", config=CONFIG)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Making dataset and loader\n",
    "    ds = CommonVoiceUkr(TRAIN_REC_PATH, TRAIN_REC_SPEC_PATH, batch_size=BATCH_SIZE)\n",
    "    train_dataloader = DataLoader(ds, shuffle=True, collate_fn=collate_fn, batch_size=BATCH_SIZE)\n",
    "    train_val_dataloader = DataLoader(ds, shuffle=True, collate_fn=collate_fn, batch_size=64)\n",
    "\n",
    "    epochs = CONFIG[\"epochs\"]\n",
    "    train_len = len(train_dataloader) * epochs\n",
    "\n",
    "    tgt_n = 152\n",
    "    d_model = 64\n",
    "    model = Model(d_model=d_model, \n",
    "                  n_encoders=CONFIG[\"n_encoders\"], \n",
    "                  n_decoders=CONFIG[\"n_decoders\"], \n",
    "                  device=device)\n",
    "    if CONFIG[\"pretrain\"] == True:\n",
    "        PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "        model = Model(n_encoders=CONFIG[\"n_encoders\"], n_decoders=CONFIG[\"n_decoders\"], device=device)\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    # Create optimizator\n",
    "    optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "    save_model = False\n",
    "    scheduler = get_scheduler(CONFIG[\"epochs\"], train_len, optimizer, scheduler_name=\"constant\", wb=wandb_stat)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch №{epoch}\")\n",
    "        train(model, train_dataloader, optimizer, device, scheduler=scheduler, epoch=epoch, wb=wandb_stat)\n",
    "        val(model, train_val_dataloader, device, epoch, wb=wandb_stat)\n",
    "        scheduler.step(epoch)\n",
    "        print(f\"scheduler last_lr: {scheduler.get_last_lr()[0]}\")\n",
    "        if wandb_stat:\n",
    "            wandb_stat.log({\"scheduler lr\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "    if save_model:\n",
    "        PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "        print(f\"Save model to path: '{PATH}'\")\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba92347-6de7-44e9-b51b-de667b4aacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "a = torch.randn(25, 300)\n",
    "b = torch.randn(22, 300)\n",
    "c = torch.randn(15, 300)\n",
    "pad_sequence((a, b, c)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68075622-b035-47b6-8f8d-8bc39773c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {\"a\": 1, \"b\": 2}\n",
    "d2 = {\"a\": 3, \"b\": 4}\n",
    "\n",
    "ds = (d1, d2)\n",
    "d_out = {}\n",
    "\n",
    "for key in d1.keys():\n",
    "    d_out[key] = [d[key] for d in ds]\n",
    "\n",
    "print(d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11befa2a-5e13-40b1-82db-20930e4fd8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inputs shape: torch.Size([34, 4, 38])\n",
    "Tgt shape: torch.Size([4, 36])\n",
    "Input length shape: torch.Size([4])\n",
    "Tgt length shape: torch.Size([4])\n",
    "\"\"\"\n",
    "\n",
    "T = 34      # Input sequence length\n",
    "C = 38      # Number of classes (including blank)\n",
    "N = 4      # Batch size\n",
    "S = 36      # Target sequence length of longest target in batch (padding length)\n",
    "\n",
    "ctc_loss = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada431e-5d58-4682-b63b-f18e30dd9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(T, N, C).requires_grad_()\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.full(size=(N,), fill_value=S, dtype=torch.long)\n",
    "\n",
    "print(f\"Inputs shape: {input.shape, input}\")\n",
    "print(f\"Tgt shape: {target.shape}\")\n",
    "print(f\"Input length shape: {input_lengths.shape}\")\n",
    "print(f\"Tgt length shape: {target_lengths.shape}\")\n",
    "\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338d204-e897-4093-9b59-5b01bba355ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_inputs = 768\n",
    "d_model = 64\n",
    "\n",
    "X = torch.randn([1, 768, 14])\n",
    "conv1 = nn.Sequential(\n",
    "    nn.Conv1d(d_inputs, d_model, kernel_size=7, stride=3),\n",
    "    nn.ReLU())\n",
    "conv2 = nn.Sequential(\n",
    "    nn.Conv1d(256, d_model, kernel_size=7, stride=3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "out = conv1(X)\n",
    "print(f\"shape after conv1: {out.shape=}\")\n",
    "#out = conv2(out)\n",
    "#print(f\"shape after conv2: {out.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892e123-6475-460c-941a-325b9915c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Conformer as con\n",
    "from data_processing import ukr_lang_chars_handle\n",
    "from data_processing import CommonVoiceUkr\n",
    "from config import *\n",
    "from torch.optim import RAdam, AdamW\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "import numpy as np\n",
    "#from torch.optim.lr_scheduler import MultiplicativeLR\n",
    "from model import MaskedSoftmaxCELoss\n",
    "from model import get_cosine_schedule_with_warmup\n",
    "import wandb\n",
    "\n",
    "# wandb.init(project=\"ASR\", entity=\"alex2135\")\n",
    "\n",
    "# wandb.config = CONFIG\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Making dataset and loader\n",
    "ds = CommonVoiceUkr(TRAIN_PATH, TRAIN_SPEC_PATH, batch_size=BATCH_SIZE)\n",
    "train_dataloader = DataLoader(ds, shuffle=True, batch_size=BATCH_SIZE)\n",
    "train_len = len(train_dataloader) * CONFIG[\"epochs\"]\n",
    "print(\"train len:\", train_len)\n",
    "\n",
    "def eleminate_channels(X: torch.Tensor) -> torch.Tensor:\n",
    "    b, c, h, w = X.shape\n",
    "    X = X.view(b, c*h, w)\n",
    "    return X\n",
    "\n",
    "tgt_n = 152\n",
    "model = con(n_encoders=CONFIG[\"n_encoders\"], n_decoders=CONFIG[\"n_decoders\"], device=device)\n",
    "if CONFIG[\"pretrain\"] == True:\n",
    "    PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "    model = con(n_encoders=CONFIG[\"n_encoders\"], n_decoders=CONFIG[\"n_decoders\"], device=device)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# Create optimizator\n",
    "optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=train_len//5, num_training_steps=train_len)\n",
    "\n",
    "# Create CTC criterion\n",
    "alpha_loss = torch.Tensor([0.7]).to(device)\n",
    "ctc_criterion = nn.CTCLoss(blank=ukr_lang_chars_handle.token_to_index[\"<blank>\"], zero_infinity=True)\n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "running_loss = []\n",
    "losses_per_phase = []\n",
    "epochs = CONFIG[\"epochs\"]\n",
    "try:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch №{epoch}\")\n",
    "        for idx, (X, tgt) in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "            tgt_text = tgt[\"text\"]\n",
    "            tgt_class = tgt[\"label\"]\n",
    "\n",
    "            one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt_text, 152).to(device)\n",
    "            X = X.to(device) #\n",
    "\n",
    "            emb, output = model(X, one_hots)  # (batch, _, n_class, time), (batch, _, time, n_class)\n",
    "            b, cnls, t, clss = output.shape\n",
    "            output = output.view(t * cnls, b, clss)  # (time, batch, n_class)\n",
    "            output = F.log_softmax(output, dim=-1)\n",
    "            indeces = ukr_lang_chars_handle.sentences_to_indeces(tgt_text).to(device)\n",
    "\n",
    "            input_lengths = torch.full(size=(BATCH_SIZE,), fill_value=t, dtype=torch.long).to(device)\n",
    "            target_lengths = torch.full(size=(BATCH_SIZE,), fill_value=indeces.shape[-1], dtype=torch.long).to(device)\n",
    "            ctc_loss = ctc_criterion(output.to(device), indeces, input_lengths, target_lengths)\n",
    "\n",
    "            print(f\"{output.shape=}\")\n",
    "            break\n",
    "            #print(f\"\\n{emb.shape=}\\n{one_hots.shape=}\")\n",
    "            #emb, one_hots = eleminate_channels(emb.to(device)), eleminate_channels(one_hots.float())\n",
    "            #ce_loss = ce_criterion(emb, one_hots)\n",
    "\n",
    "            #loss = alpha_loss * torch.log(ce_loss) + (1-alpha_loss) * torch.log(ctc_loss)\n",
    "            loss = ctc_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            #print(f\"\\n{ctc_loss.item()=}, {torch.log(ctc_loss)}\\n{ce_loss.item()=}, {torch.log(ce_loss)}\")\n",
    "            running_loss.append(loss.cpu().detach().numpy())\n",
    "            losses_per_phase.append(loss.cpu().detach().numpy())\n",
    "            # wandb.log({\"loss\": loss})\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(\"Target label:\", tgt)\n",
    "                print(\"Running loss:\")\n",
    "                pprint.pprint(running_loss)\n",
    "                print(output.shape)\n",
    "                print(\"Is nan in output:\", torch.sum(torch.isnan(output)))\n",
    "                print(\"Is inf in output:\", torch.sum(torch.isinf(output)))\n",
    "                pprint.pprint(output)\n",
    "                break\n",
    "            if (idx + 1) % 50 == 0:  # print every 200 mini-batches\n",
    "                print(f\"Epoch: {epoch}, Last loss: {loss.item():.4f}, Loss phase mean: {np.mean(np.array(losses_per_phase)):.4f}\")\n",
    "                losses_per_phase = []\n",
    "            optimizer.zero_grad()\n",
    "    import os\n",
    "    PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "    print(PATH)\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5684ef0-3bdc-42ae-820a-a6addd88c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfd29903-e0dd-49a6-977a-f52f4f9ffd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from config import *\n",
    "from data_processing import ukr_lang_chars_handle\n",
    "from data_processing import UkrVoiceDataset\n",
    "from model import EfConfRecognizer as Model\n",
    "from model import get_cosine_schedule_with_warmup, OneCycleLR\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, optimizer, device, scheduler=None, epoch=1, wb=None):\n",
    "    print(f\"Training begin\")\n",
    "    model.train()\n",
    "    ctc_criterion = nn.CTCLoss(reduction=\"none\", blank=ukr_lang_chars_handle.token_to_index[\"<blank>\"], zero_infinity=True)\n",
    "    running_loss = []\n",
    "    losses_per_phase = []\n",
    "    train_len = len(train_dataloader)\n",
    "\n",
    "    for idx, (X, tgt) in tqdm(enumerate(train_dataloader)):\n",
    "        tgt_text = tgt[\"text\"]\n",
    "\n",
    "        tgt_lengths = [len(txt) for txt in tgt_text]\n",
    "        tgt_max_len = max(tgt_lengths)\n",
    "        one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt_text, tgt_max_len).to(device)\n",
    "        one_hots = one_hots.squeeze(dim=1).permute(0, 2, 1).float()\n",
    "        \n",
    "        X = X.to(device) #\n",
    "        X = X.squeeze(dim=1).permute(0, 2, 1)\n",
    "        #print(f\"{X.shape=}\")\n",
    "        #print(f\"{one_hots.shape=}\")\n",
    "\n",
    "        emb, output = model(X, one_hots)  # (batch, time, n_class), (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        \n",
    "        output = output.permute(1, 0, 2).to(device).detach().requires_grad_()\n",
    "        indeces = ukr_lang_chars_handle.sentences_to_indeces(tgt_text).to(device)\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        print(\"tgt_text:\")\n",
    "        pprint(tgt_text)\n",
    "        print(\"tgt_len\")\n",
    "        pprint(tgt_lengths)\n",
    "        print(\"indeces:\")\n",
    "        pprint(indeces)\n",
    "        print(f\"Inputs shape: {output.shape}\")\n",
    "        print(f\"Tgt shape: {indeces.shape}\")\n",
    "        print(f\"one_hots shape: {one_hots.shape}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        input_lengths = torch.full(size=(output.shape[1],), fill_value=output.shape[0], dtype=torch.long).to(device)\n",
    "        target_lengths = torch.Tensor(tgt_lengths).long().to(device)        \n",
    "        \n",
    "        loss = ctc_criterion(output, indeces, input_lengths, target_lengths)\n",
    "        #print(f\"{loss=}\")\n",
    "        if wb:\n",
    "            wb.log({\n",
    "                \"loss\": loss.item(),\n",
    "                \"epoch\": epoch\n",
    "            })\n",
    "        #print(f\"{loss=}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "        losses_per_phase.append(loss.cpu().detach().numpy())\n",
    "        if (idx + 1) % 10 == 0:  # print every 200 mini-batches\n",
    "            loss_mean = np.mean(np.array(losses_per_phase))\n",
    "            print(f\"Epoch: {epoch}, Last loss: {loss.item():.4f}, Loss phase mean: {loss_mean:.4f}\")\n",
    "            if wb:\n",
    "                wb.log({\"loss phase mean\": loss_mean})\n",
    "            losses_per_phase = []\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60bc1cbc-5bb4-4451-9ed1-158c3651a725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch №1\n",
      "Training begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 113.0288, Loss phase mean: 166.0477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:05,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 129.7336, Loss phase mean: 87.8743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:06,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 309.3916, Loss phase mean: 158.7272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [00:08,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 224.2455, Loss phase mean: 146.5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:10,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 48.8244, Loss phase mean: 125.6323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:12,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 0.0000, Loss phase mean: 112.5623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:13,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 312.2733, Loss phase mean: 85.1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [00:15,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 0.0000, Loss phase mean: 148.5828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:17,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 0.0000, Loss phase mean: 116.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:19,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 254.6492, Loss phase mean: 101.4529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [00:20,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 140.0344, Loss phase mean: 159.9665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [00:23,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 114.7276, Loss phase mean: 156.6473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "130it [00:26,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 228.6043, Loss phase mean: 153.3013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141it [00:28,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 278.9314, Loss phase mean: 130.8798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "151it [00:30,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 221.3641, Loss phase mean: 114.4657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "160it [00:33,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 49.5900, Loss phase mean: 107.6594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "171it [00:34,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 0.0000, Loss phase mean: 105.9473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180it [00:36,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 0.0000, Loss phase mean: 137.2498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [00:38,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 427.1979, Loss phase mean: 183.2187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:41,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 241.9095, Loss phase mean: 124.7030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "209it [00:42,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 44.6705, Loss phase mean: 156.5662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "219it [00:44,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 18.0213, Loss phase mean: 80.5354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "230it [00:47,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 158.6105, Loss phase mean: 118.8133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:50,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 101.4234, Loss phase mean: 153.4497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250it [00:52,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 399.4970, Loss phase mean: 121.3272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "260it [00:54,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 103.9547, Loss phase mean: 95.8644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "270it [00:56,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 66.8784, Loss phase mean: 137.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "281it [00:59,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 85.3929, Loss phase mean: 134.0541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "290it [01:01,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 88.1944, Loss phase mean: 123.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [01:03,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 0.0000, Loss phase mean: 116.7593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "311it [01:05,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Last loss: 113.0333, Loss phase mean: 181.8001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "311it [01:06,  4.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15980/2551068961.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15980/2551068961.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch №{epoch}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwandb_stat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_val_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwandb_stat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15980/788375747.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, optimizer, device, scheduler, epoch, wb)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m#print(f\"{one_hots.shape=}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hots\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch, time, n_class), (batch, time, n_class)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\efficient_conformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, tgt)\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlin_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\efficient_conformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, tgt)\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_proc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_enc_inp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec_proc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\efficient_conformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mpe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\Speech recognition\\NLP_diploma\\ASR proto\\ASR proto\\model\\efficient_conformer.py\u001b[0m in \u001b[0;36mposition_encoding\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mpe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mpe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def val(model, train_dataloader, device, epoch, wb=None):\n",
    "    model.eval()\n",
    "    positive = 0\n",
    "    train_len = train_dataloader.sampler.num_samples\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Evaluation on train dataset\")\n",
    "    with torch.no_grad():\n",
    "        for idx, (X, tgt) in tqdm(enumerate(train_dataloader)):\n",
    "            tgt_text = \" \"#tgt[\"text\"]\n",
    "            tgt_class = torch.Tensor(tgt[\"label\"]).long().to(device)\n",
    "            tgt_class = F.one_hot(tgt_class, num_classes=5)\n",
    "            one_hots = ukr_lang_chars_handle.sentences_to_one_hots(tgt_text, 152).to(device)\n",
    "            one_hots = one_hots.squeeze(dim=1).permute(0, 2, 1).float()\n",
    "            X = X.to(device)  #\n",
    "            X = X.squeeze(dim=1).permute(0, 2, 1)\n",
    "            emb, output = model(X, one_hots)\n",
    "            A = torch.argmax(output, dim=-1)\n",
    "            B = torch.argmax(tgt_class, dim=-1)\n",
    "            is_right = (A == B)\n",
    "            positive += torch.sum(is_right)\n",
    "\n",
    "    train_accuracy = positive / train_len\n",
    "    if wb:\n",
    "        wb.log({\n",
    "            \"train accuracy\": train_accuracy,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "    print(f\"Accuracy on TRAIN dataset: {train_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "\n",
    "def get_scheduler(epochs, train_len, optimizer, scheduler_name=\"cosine_with_warmup\", wb=None):\n",
    "    if wb:\n",
    "        wb.config[\"scheduler\"] = scheduler_name\n",
    "    if scheduler_name == \"cosine_with_warmup\":\n",
    "        return get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=epochs//5,\n",
    "                                                num_training_steps=epochs - epochs//5,\n",
    "                                                num_cycles=0.5)#1.25)\n",
    "    elif scheduler_name == \"constant\":\n",
    "        return torch.optim.lr_scheduler.ConstantLR(optimizer)\n",
    "    elif scheduler_name == \"exponential\":\n",
    "        return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "    elif scheduler_name == \"one_circle\":\n",
    "        return OneCycleLR(optimizer,\n",
    "                          max_lr=CONFIG[\"learning_rate\"]*10,\n",
    "                          total_steps=train_len)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    Xs, LBLs = zip(*data)\n",
    "    Xs_out = pad_sequence([X.permute(0, 2, 1).squeeze(dim=0) for X in Xs], batch_first=True)\n",
    "    lbl1 = LBLs[0]\n",
    "    d_out = {}\n",
    "    for key in lbl1.keys():\n",
    "        d_out[key] = [d[key] for d in LBLs]\n",
    "    return Xs_out, d_out\n",
    "\n",
    "\n",
    "def main():\n",
    "    wandb_stat = None#wandb.init(project=\"ASR\", entity=\"Alex2135\", config=CONFIG)\n",
    "    device = torch.device(\"cuda\") if not torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    # Making dataset and loader\n",
    "    ds = UkrVoiceDataset(TRAIN_REC_PATH, TRAIN_REC_SPEC_PATH)\n",
    "    train_dataloader = DataLoader(ds, shuffle=True, collate_fn=collate_fn, batch_size=CONFIG[\"batch_size\"][\"train\"])\n",
    "    train_val_dataloader = DataLoader(ds, shuffle=True, collate_fn=collate_fn, batch_size=CONFIG[\"batch_size\"][\"test\"])\n",
    "\n",
    "    epochs = CONFIG[\"epochs\"]\n",
    "    train_len = len(train_dataloader) * epochs\n",
    "\n",
    "    tgt_n = 152\n",
    "    d_model = 64\n",
    "    model = Model(d_model=d_model, \n",
    "                  n_encoders=CONFIG[\"n_encoders\"], \n",
    "                  n_decoders=CONFIG[\"n_decoders\"], \n",
    "                  device=device)\n",
    "\n",
    "    # Create optimizator\n",
    "    optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "    save_model = False\n",
    "    scheduler = get_scheduler(CONFIG[\"epochs\"], train_len, optimizer, scheduler_name=\"constant\", wb=wandb_stat)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch №{epoch}\")\n",
    "        train(model, train_dataloader, optimizer, device, scheduler=scheduler, epoch=epoch, wb=wandb_stat)\n",
    "        val(model, train_val_dataloader, device, epoch, wb=wandb_stat)\n",
    "        scheduler.step(epoch)\n",
    "        print(f\"scheduler last_lr: {scheduler.get_last_lr()[0]}\")\n",
    "        if wandb_stat:\n",
    "            wandb_stat.log({\"scheduler lr\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "    if save_model:\n",
    "        PATH = os.path.join(DATA_DIR, \"model_1.pt\")\n",
    "        print(f\"Save model to path: '{PATH}'\")\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba92347-6de7-44e9-b51b-de667b4aacfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3, 300])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "a = torch.randn(25, 300)\n",
    "b = torch.randn(22, 300)\n",
    "c = torch.randn(15, 300)\n",
    "pad_sequence((a, b, c)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68075622-b035-47b6-8f8d-8bc39773c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {\"a\": 1, \"b\": 2}\n",
    "d2 = {\"a\": 3, \"b\": 4}\n",
    "\n",
    "ds = (d1, d2)\n",
    "d_out = {}\n",
    "\n",
    "for key in d1.keys():\n",
    "    d_out[key] = [d[key] for d in ds]\n",
    "\n",
    "print(d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11befa2a-5e13-40b1-82db-20930e4fd8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inputs shape: torch.Size([34, 4, 38])\n",
    "Tgt shape: torch.Size([4, 36])\n",
    "Input length shape: torch.Size([4])\n",
    "Tgt length shape: torch.Size([4])\n",
    "\"\"\"\n",
    "\n",
    "T = 52      # Input sequence length\n",
    "C = 38      # Number of classes (including blank)\n",
    "N = 4      # Batch size\n",
    "S = 36      # Target sequence length of longest target in batch (padding length)\n",
    "\n",
    "ctc_loss = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aada431e-5d58-4682-b63b-f18e30dd9c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([52, 4, 38])\n",
      "Tgt shape: torch.Size([4, 36])\n",
      "Input length shape: torch.Size([4])\n",
      "Tgt length shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(T, N, C).requires_grad_()\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.full(size=(N,), fill_value=S, dtype=torch.long)\n",
    "\n",
    "print(f\"Inputs shape: {input.shape}\")\n",
    "print(f\"Tgt shape: {target.shape}\")\n",
    "print(f\"Input length shape: {input_lengths.shape}\")\n",
    "print(f\"Tgt length shape: {target_lengths.shape}\")\n",
    "\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a5e80a-f13d-4b9f-bf3f-cc2ff04f65ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4048)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "X = torch.Tensor([[0, 0, 1, 0, 0], [0, 0, 0, 0, 1]])\n",
    "tgt = torch.Tensor([[0, 0, 1, 0, 0], [0, 1, 0, 0, 0]])\n",
    "\n",
    "loss = ce_criterion(X, tgt)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338d204-e897-4093-9b59-5b01bba355ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_inputs = 768\n",
    "d_model = 64\n",
    "\n",
    "X = torch.randn([1, 768, 14])\n",
    "conv1 = nn.Sequential(\n",
    "    nn.Conv1d(d_inputs, d_model, kernel_size=7, stride=3),\n",
    "    nn.ReLU())\n",
    "conv2 = nn.Sequential(\n",
    "    nn.Conv1d(256, d_model, kernel_size=7, stride=3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "out = conv1(X)\n",
    "print(f\"shape after conv1: {out.shape=}\")\n",
    "#out = conv2(out)\n",
    "#print(f\"shape after conv2: {out.shape=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

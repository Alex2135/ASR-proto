{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1f04e9-416a-4711-b2d6-d999d486f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c93be7-f14b-499f-bfcc-431408c39ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightAttention(nn.Module):\n",
    "    def __init__(self, dim_input_q, dim_input_kv, dim_q, dim_k, device=\"cpu\", with_mask=False):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.with_mask = with_mask\n",
    "        self.softmax_col = nn.Softmax(dim=-1)\n",
    "        self.softmax_row = nn.Softmax(dim=-2)\n",
    "        self.W_q = nn.Linear(in_features=dim_input_q, out_features=dim_q)\n",
    "        self.W_k = nn.Linear(in_features=dim_input_kv, out_features=dim_k)\n",
    "        self.W_v = nn.Linear(in_features=dim_input_kv, out_features=dim_k)\n",
    "        self.d_q = torch.pow(torch.Tensor([dim_q]).to(device), 1/4)\n",
    "        self.d_k = torch.pow(torch.Tensor([dim_k]).to(device), 1/4)\n",
    "\n",
    "    def mask(self, dim: (int, int)) -> Tensor :\n",
    "        a, b = dim\n",
    "        mask = torch.ones(b, a)\n",
    "        mask = torch.triu(mask, diagonal=0)\n",
    "        mask = torch.log(mask.T)\n",
    "        return mask.to(self.device)\n",
    "        \n",
    "    def forward(self, x_q, x_k, x_v):\n",
    "        Q = self.W_q(x_q)\n",
    "        K = self.W_k(x_k)\n",
    "        V = self.W_v(x_v)\n",
    "        if self.with_mask == True:\n",
    "            Q += self.mask(Q.shape[-2:])\n",
    "        A = self.softmax_row(Q / self.d_q)\n",
    "        B = torch.matmul(self.softmax_col(K.transpose(-2, -1) / self.d_k), V)\n",
    "        Z = torch.matmul(A, B)\n",
    "        return Z\n",
    "\n",
    "\n",
    "class MHLA(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_heads, \n",
    "                 dim_input_q,\n",
    "                 dim_input_kv,\n",
    "                 dim_q = 64,\n",
    "                 dim_k = 64,\n",
    "                 device=\"cpu\",\n",
    "                 mask=False\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        dim_input - if shape is (B, C, H, W), then dim_input is W\n",
    "        \"\"\"\n",
    "        #TODO: Make parallel heads like channel dim\n",
    "        super().__init__()\n",
    "        heads = [LightAttention(dim_input_q, dim_input_kv, dim_q, dim_k, device, mask) for _ in range(num_heads)]\n",
    "        self.heads = nn.ModuleList(heads)                \n",
    "        self.W_o = nn.Linear(dim_k*num_heads, dim_input_kv)\n",
    "        \n",
    "    def forward(self, x_q, x_k, x_v):\n",
    "        x = torch.cat([latt(x_q, x_k, x_v) for latt in self.heads], dim=-1)\n",
    "        y = self.W_o(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe46743c-1c97-4eac-806c-09882d9c674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape=torch.Size([2, 1, 64, 256])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#x_q.shape=torch.Size([2, 1, 64, 256])\n",
    "X = torch.randn(2, 1, 64, 256)\n",
    "\n",
    "mhla = MHLA(6, 256, 256)\n",
    "y = mhla(X, X, X)\n",
    "print(f\"{y.shape=}\")\n",
    "print(f\"{y.shape==X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b52be6b7-fba2-4de8-b602-b2f709f1be7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.shape=torch.Size([2, 4, 256, 16])\n",
      "K.shape=torch.Size([2, 4, 256, 16])\n",
      "V.shape=torch.Size([2, 4, 256, 16])\n",
      "A.shape=torch.Size([2, 4, 16, 16])\n",
      "B.shape=torch.Size([2, 4, 256, 16])\n",
      "y.shape=torch.Size([2, 1, 64, 256])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "class MHLA2(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_heads, \n",
    "                 dim_last_input_q,\n",
    "                 dim_last_input_kv,\n",
    "                 dim_q = 16,\n",
    "                 dim_k = 16,\n",
    "                 device=\"cpu\",\n",
    "                 mask=False\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \n",
    "        \"\"\"\n",
    "        #TODO: Make parallel heads like channel dim\n",
    "        super().__init__()\n",
    "        # self.W_Q = nn.Linear(dim_last_input_q, dim_q, bias=False)\n",
    "        # self.W_K = nn.Linear(dim_last_input_kv, dim_k, bias=False)\n",
    "        # self.W_V = nn.Linear(dim_last_input_kv, dim_k, bias=False)      \n",
    "        self.with_mask = mask\n",
    "        self.W_Q = torch.ones((num_heads, dim_last_input_q, dim_q), device=device, requires_grad=True)\n",
    "        self.W_K = torch.ones((num_heads, dim_last_input_q, dim_q), device=device, requires_grad=True)\n",
    "        self.W_V = torch.ones((num_heads, dim_last_input_q, dim_q), device=device, requires_grad=True)\n",
    "        self.W_O = nn.Linear(dim_k*num_heads, dim_k*num_heads, bias=False)\n",
    "        nn.init.xavier_uniform_(self.W_Q)\n",
    "        nn.init.xavier_uniform_(self.W_K)\n",
    "        nn.init.xavier_uniform_(self.W_V)\n",
    "        self.d_q = torch.pow(torch.Tensor([dim_q]).to(device), 1/4)\n",
    "        self.d_k = torch.pow(torch.Tensor([dim_k]).to(device), 1/4)\n",
    "        self.softmax_col = nn.Softmax(dim=-1)\n",
    "        self.softmax_row = nn.Softmax(dim=-2)\n",
    "        \n",
    "    def mask(self, dim: (int, int)) -> Tensor :\n",
    "        a, b = dim\n",
    "        mask = torch.ones(b, a)\n",
    "        mask = torch.triu(mask, diagonal=0)\n",
    "        mask = torch.log(mask.T)\n",
    "        return mask.to(self.device)\n",
    "        \n",
    "    def forward(self, x_q, x_k, x_v):\n",
    "        Q = torch.matmul(x_q.transpose(-1, -2).contiguous(), self.W_Q)\n",
    "        K = torch.matmul(x_k.transpose(-1, -2).contiguous(), self.W_K)\n",
    "        V = torch.matmul(x_v.transpose(-1, -2).contiguous(), self.W_V)\n",
    "        if self.with_mask == True:\n",
    "            Q += self.mask(Q.shape[-2:])\n",
    "        print(f\"{Q.shape=}\")\n",
    "        print(f\"{K.shape=}\")\n",
    "        print(f\"{V.shape=}\")\n",
    "        A = torch.matmul(self.softmax_col(K.transpose(-1, -2).contiguous() / self.d_k), V)\n",
    "        B = torch.matmul(self.softmax_row(Q / self.d_q), A)\n",
    "        #A = torch.matmul(Q, K.transpose(-1, -2).contiguous())\n",
    "        print(f\"{A.shape=}\")\n",
    "        #B = torch.matmul(A, V)\n",
    "        print(f\"{B.shape=}\")\n",
    "        b, h, w, d = B.shape\n",
    "        B = self.W_O(B.view(b, w, h*d))\n",
    "        B = B.unsqueeze(dim=1).permute(0,1,3,2)\n",
    "        \n",
    "        return B\n",
    "    \n",
    "    \n",
    "X = torch.randn(2, 1, 64, 256)\n",
    "\n",
    "mhla2 = MHLA2(4, 64, 64)\n",
    "y = mhla2(X, X, X)\n",
    "print(f\"{y.shape=}\")\n",
    "print(f\"{y.shape==X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6abd1e92-58f9-4989-a685-dd7cc7911948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8611,  0.2410,  0.3465,  1.3166],\n",
       "         [-1.1952,  1.1787, -1.0054,  0.7407],\n",
       "         [-0.7926,  0.2395, -0.8082, -1.3719]],\n",
       "\n",
       "        [[ 0.0121, -0.0247,  1.6018,  1.1075],\n",
       "         [ 0.0031,  0.2784, -2.0812, -2.1760],\n",
       "         [-0.5154,  0.3037,  2.8692, -0.0420]]], requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fe8bade-78c5-4213-ae8e-d5d67654f484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
    "out = x.pow(2).sum()\n",
    "out.backward()\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7509c41-4793-45b1-ab06-bf14c763a5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c45ad9-c21b-41e6-bd36-5dc6ca325ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a224ee5b-7f8a-4c21-8210-11effc0ac62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER PARAMS\n",
    "# Encoder inputs\n",
    "enc_n_feats = 256   # spectrogram height\n",
    "enc_d_model = 1024  # spectrogram max width\n",
    "\n",
    "# DECODER PARAMS\n",
    "# Decoder inputs--\n",
    "dec_n_feats = 38    # len(tokens)\n",
    "dec_d_model = 152   # maximum symbols in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5d1bfd19-6fd8-48d9-919a-65f9e6428cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * x.sigmoid()\n",
    "\n",
    "    \n",
    "class AbsolutePositionEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def pos_f(self, row, column, emb_dim):\n",
    "        func = (np.sin, np.cos)[row % 2]\n",
    "        w_k = 1/np.power(10000, 2*row/emb_dim)\n",
    "        pe_i_j = func(column * w_k)\n",
    "        return torch.Tensor([pe_i_j])\n",
    "    \n",
    "    def position_encoding(self, X):\n",
    "        b, _, h, w = X.shape\n",
    "        pe = torch.zeros((b, h, w))\n",
    "        for k in range(b):\n",
    "            for i in range(h):\n",
    "                for j in range(w):\n",
    "                    pe[k][i][j] = self.pos_f(i, j, h)\n",
    "                \n",
    "        pe = pe.reshape(b, 1, h, w)\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        PE = self.position_encoding(x)\n",
    "        return PE\n",
    "                \n",
    "                \n",
    "class LFFN(nn.Module):\n",
    "    def __init__(self, inputs_dim, dim_hid):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        inputs_dim - tuple, \n",
    "            (N, C, H, W) of inputs\n",
    "        dim_hid - int, \n",
    "            number of hidden units\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        _, _, dim_bn, dim = inputs_dim\n",
    "        self.E1 = nn.Linear(in_features=dim, out_features=dim_bn, bias=False)\n",
    "        self.D1 = nn.Linear(in_features=dim_bn, out_features=dim_hid, bias=False)\n",
    "        self.swish = Swish()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.E2 = nn.Linear(in_features=dim_hid, out_features=dim_bn, bias=False)\n",
    "        self.D2 = nn.Linear(in_features=dim_bn, out_features=dim, bias=False)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.E1(inputs)\n",
    "        x = self.D1(x)\n",
    "        x = self.swish(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.E2(x)\n",
    "        y = self.D2(x)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class LightAttention(nn.Module):\n",
    "    def __init__(self, dim_input_q, dim_input_kv, dim_q, dim_k, with_mask=False):\n",
    "        super().__init__()\n",
    "        self.with_mask = with_mask\n",
    "        self.softmax_col = nn.Softmax(dim=-1)\n",
    "        self.softmax_row = nn.Softmax(dim=-2)\n",
    "        \n",
    "        self.W_q = nn.Linear(in_features=dim_input_q, out_features=dim_q)\n",
    "        self.W_k = nn.Linear(in_features=dim_input_kv, out_features=dim_k)\n",
    "        self.W_v = nn.Linear(in_features=dim_input_kv, out_features=dim_k)\n",
    "        \n",
    "        self.d_q = torch.pow(torch.Tensor([dim_q]), 1/4)\n",
    "        self.d_k = torch.pow(torch.Tensor([dim_k]), 1/4)\n",
    "        \n",
    "    def mask(self, dim: (int, int)) -> Tensor :\n",
    "        a, b = dim\n",
    "        mask = torch.ones(b, a)\n",
    "        mask = torch.triu(mask, diagonal=0)\n",
    "        mask = torch.log(mask.T)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, x_q, x_k, x_v):\n",
    "        Q = self.W_q(x_q)\n",
    "        K = self.W_k(x_k)\n",
    "        V = self.W_v(x_v)\n",
    "        \n",
    "        if self.with_mask == True:\n",
    "            Q += self.mask(Q.shape[-2:])\n",
    "        \n",
    "        A = self.softmax_row(Q / self.d_q)\n",
    "        B = torch.matmul(self.softmax_col(K.transpose(-2, -1) / self.d_k), V)\n",
    "        Z = torch.matmul(A, B)\n",
    "        \n",
    "        return Z\n",
    "\n",
    "\n",
    "class MHLA(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_heads, \n",
    "                 dim_input_q,\n",
    "                 dim_input_kv,\n",
    "                 dim_q = 64,\n",
    "                 dim_k = 64,\n",
    "                 mask=False\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        dim_input - if shape is (B, C, H, W), then dim_input is W\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        heads = [LightAttention(dim_input_q, dim_input_kv, dim_q, dim_k, mask) for _ in range(num_heads)]\n",
    "        self.heads = nn.ModuleList(heads)                \n",
    "        self.W_o = nn.Linear(dim_k*num_heads, dim_input_kv)\n",
    "        \n",
    "    def forward(self, x_q, x_k, x_v):\n",
    "        x = torch.cat([latt(x_q, x_k, x_v) for latt in self.heads], dim=-1)\n",
    "        y = self.W_o(x)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, gate = x.chunk(2, dim=self.dim)\n",
    "        return out * gate.sigmoid()\n",
    "    \n",
    "    \n",
    "class DepthWiseConv1d(nn.Module):\n",
    "    def __init__(self, chan_in, chan_out, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(chan_in, chan_in, kernel_size=(1, kernel_size), padding=(0, padding))\n",
    "        self.conv2 = nn.Conv2d(chan_in, chan_out, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PointWiseConv(nn.Module):\n",
    "    def __init__(self, chan_in):\n",
    "        super().__init__()\n",
    "        self.pw_conv = nn.Conv2d(in_channels=chan_in, out_channels=1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.pw_conv(inputs)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, dim_C, dim_H, dim_W, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm([dim_H, dim_W*dim_C])\n",
    "        self.pw_conv1 = PointWiseConv(chan_in=dim_C)\n",
    "        self.glu = GLU(-2)\n",
    "        self.dw_conv1d = DepthWiseConv1d(dim_C, dim_C*2, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(dim_C)\n",
    "        self.swish = Swish()\n",
    "        self.pw_conv2 = PointWiseConv(chan_in=dim_C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.reshape(b, h, w*c)\n",
    "        \n",
    "        x = self.ln1(x)\n",
    "        x = x.reshape(b, c, h, w)\n",
    "        \n",
    "        x = self.pw_conv1(x)\n",
    "        x = self.glu(x)\n",
    "        x = self.dw_conv1d(x)\n",
    "        x = x.reshape(b, c, -1, w)\n",
    "        \n",
    "        x = self.bn(x)\n",
    "        x = self.swish(x)\n",
    "        x = self.pw_conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class LAC(nn.Module):\n",
    "    def __init__(self, dim_B=1, dim_C=1, dim_H=64, dim_W=256):\n",
    "        super().__init__()\n",
    "        self.lffn1 = LFFN(inputs_dim=(dim_B, dim_C, dim_H, dim_W), dim_hid=1024)\n",
    "        self.mhlsa = MHLA(num_heads=4, dim_input_q=dim_W, dim_input_kv=dim_W)    \n",
    "        self.conv_module = ConvModule(dim_C, dim_H, dim_W)\n",
    "        self.lffn2 = LFFN(inputs_dim=(dim_B, dim_C, dim_H, dim_W), dim_hid=1024)\n",
    "        self.ln = nn.LayerNorm([dim_C, dim_H, dim_W])\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = x + 1/2 * self.lffn1(x)\n",
    "        x = x + self.mhlsa(x, x, x)\n",
    "        x = x + self.conv_module(x)\n",
    "        x = x + 1/2 * self.lffn2(x)\n",
    "        x = self.ln(x)\n",
    "        return inputs + x\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, lacs_n=2):\n",
    "        super().__init__()\n",
    "        self.lacs = nn.Sequential(*[LAC() for i in range(lacs_n)])\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.lacs(inputs)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, dim_shape_tgt, dim_shape_mem):\n",
    "        super().__init__()\n",
    "        dim_B, dim_C, dim_H, dim_tgt = dim_shape_tgt\n",
    "        dim_mem = dim_shape_mem[-1]\n",
    "        self.mhla_with_mask = MHLA(num_heads=2, dim_input_q=dim_tgt, dim_input_kv=dim_tgt)\n",
    "        self.ln1 = nn.LayerNorm([dim_C, dim_H, dim_tgt])\n",
    "        self.mhla_with_memory = MHLA(num_heads=2, dim_input_q=dim_tgt, dim_input_kv=dim_mem, mask=True)\n",
    "        self.ln2 = nn.LayerNorm([dim_C, dim_H, dim_mem])\n",
    "        self.lffn = LFFN(inputs_dim=(dim_B, dim_C, dim_H, dim_mem), dim_hid=1024)\n",
    "        self.ln3 = nn.LayerNorm([dim_C, dim_H, dim_mem])\n",
    "        \n",
    "    def forward(self, mem, y):\n",
    "        y = y + self.mhla_with_mask(y, y, y)\n",
    "        y = self.ln1(y)\n",
    "        y = y + self.mhla_with_memory(y, mem, mem)\n",
    "        y = self.ln2(y)\n",
    "        y = y + self.lffn(y)\n",
    "        y = self.ln3(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: + linar modules to reshape decoder target to (1, 1, 64, 256)\n",
    "        self.dec_blocks = DecoderBlock(dim_shape_tgt=(1, 1, 38, 152), dim_shape_mem=(1, 1, 38, 256))\n",
    "        self.dec_blocks1 = DecoderBlock(dim_shape_tgt=(1, 1, 38, 152), dim_shape_mem=(1, 1, 38, 256))\n",
    "        \n",
    "    def forward(self, mem, tgt):\n",
    "        y = tgt\n",
    "        y = self.dec_blocks(mem, y)\n",
    "        y = self.dec_blocks1(mem, y)\n",
    "        print(f\"Outputs shape: {y.shape}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f17a231-32b0-423c-a722-aebe0f239d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after input_preprocess: tensor([[[[-0.0876, -0.0960, -0.1383,  ..., -0.1009, -0.0930, -0.1066],\n",
      "          [-0.1115, -0.1203, -0.1131,  ..., -0.0719, -0.0952, -0.0917],\n",
      "          [-0.1087, -0.1045, -0.1353,  ..., -0.0994, -0.0794, -0.0857],\n",
      "          ...,\n",
      "          [-0.0833, -0.1275, -0.1126,  ..., -0.1012, -0.1118, -0.0886],\n",
      "          [-0.1125, -0.1299, -0.1230,  ..., -0.1233, -0.0888, -0.1110],\n",
      "          [-0.1021, -0.1036, -0.1086,  ..., -0.0877, -0.0808, -0.0915]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "after pos_enc: tensor([[[[-0.0876,  0.7455,  0.7710,  ...,  0.8940,  0.3590, -0.6130],\n",
      "          [ 0.8885,  0.6114, -0.0421,  ...,  0.2645, -0.4908, -1.0072],\n",
      "          [-0.1087,  0.4286,  0.7668,  ..., -0.8831, -1.0736, -0.9842],\n",
      "          ...,\n",
      "          [ 0.9167,  0.8725,  0.8874,  ...,  0.8988,  0.8882,  0.9114],\n",
      "          [-0.1125, -0.1299, -0.1230,  ..., -0.1233, -0.0888, -0.1110],\n",
      "          [ 0.8979,  0.8964,  0.8914,  ...,  0.9123,  0.9192,  0.9085]]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "after LACs: tensor([[[[-2.1066,  0.8287,  3.0936,  ...,  4.8158,  2.4051, -1.4933],\n",
      "          [ 4.2721, -0.4127,  0.2420,  ..., -0.3519, -0.4040, -6.3849],\n",
      "          [ 0.4933, -0.4791,  3.2381,  ..., -1.6773, -2.7265, -6.0813],\n",
      "          ...,\n",
      "          [ 1.1363,  1.7971,  1.3341,  ...,  2.1270,  1.7345,  1.3502],\n",
      "          [-2.7957, -1.7855, -2.1685,  ..., -1.8842, -1.4830, -2.5811],\n",
      "          [ 1.0000,  2.1165,  1.6584,  ...,  2.7825,  1.8710,  1.2493]]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (152) must match the size of tensor b (256) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16816/2499364730.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16816/2499364730.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, tgt)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_enc_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16816/2388420237.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mem, tgt)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdec_blocks1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Outputs shape: {y.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ml\\speech recognition\\nlp_diploma\\asr proto\\asr proto\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16816/2388420237.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mem, y)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmhla_with_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmhla_with_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlffn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (152) must match the size of tensor b (256) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, lacs_n=2):\n",
    "        super().__init__()\n",
    "        self.input_preprocess = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=2),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
    "            Swish()\n",
    "        )\n",
    "        self.pos_enc_inp = AbsolutePositionEncoding()\n",
    "        self.pos_enc_out = AbsolutePositionEncoding()\n",
    "        self.encoder = Encoder(lacs_n)\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, inputs, tgt):\n",
    "        x = self.input_preprocess(inputs)\n",
    "        print(\"after input_preprocess:\", x)\n",
    "        x = x + self.pos_enc_inp(x)\n",
    "        print(\"after pos_enc:\", x)\n",
    "        x = self.encoder(x)\n",
    "        print(\"after LACs:\", x)\n",
    "        \n",
    "        y = self.pos_enc_out(tgt)\n",
    "        y = self.decoder(x, y)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "src = torch.rand(1, 1, 256, 1024)\n",
    "tgt = torch.rand(1, 1, 38, 152)\n",
    "\n",
    "model = Model()\n",
    "result = model(src, tgt)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61918809-b4e9-42db-b86e-2a5e1687c33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(256 >> 2 == 256 / 4)\n",
    "print(1024 >> 2 == 1024 / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3ccb46-7b66-4954-9cd1-cd66cb409b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1131.3501, 1084.0927, 1065.3154,  ..., 1081.5212, 1017.0113,\n",
       "           1042.0787],\n",
       "          [1131.8226, 1081.1790, 1062.1481,  ..., 1078.3734, 1015.0567,\n",
       "           1039.3541],\n",
       "          [1136.4053, 1086.8667, 1068.2117,  ..., 1084.2784, 1019.6134,\n",
       "           1045.2657],\n",
       "          ...,\n",
       "          [1088.2290, 1044.4198, 1022.9773,  ..., 1040.7019,  977.4701,\n",
       "            998.2241],\n",
       "          [1076.0087, 1030.0990, 1010.1744,  ..., 1024.8882,  965.7994,\n",
       "            986.8787],\n",
       "          [1092.4478, 1049.4794, 1028.0149,  ..., 1046.0243,  983.2220,\n",
       "           1005.9284]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(1, 1, 64, 256)\n",
    "E = torch.rand(256, 64)\n",
    "D = torch.rand(64, 1024)\n",
    "1/2 * torch.matmul(torch.matmul(X, E), D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b23eab76-fc90-425f-9288-08dcec515c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3066,  1.0177, -0.7403],\n",
      "        [-0.1246,  1.4291, -0.0796]])\n",
      "tensor([[0.4546, 0.3986, 0.3406],\n",
      "        [0.5454, 0.6014, 0.6594]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=-1) # softmax row\n",
    "m = nn.Softmax(dim=-2) # softmax col\n",
    "input = torch.randn(2, 3)\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43a556b6-316d-4e40-881c-935dc49d1a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0318e-10, 3.1558e-10, 3.2400e-10, 2.9185e-10, 3.1034e-10, 3.1987e-10,\n",
       "        3.2264e-10, 1.9396e-11, 2.9185e-10, 2.8999e-10, 2.9092e-10, 2.9458e-10,\n",
       "        2.8416e-10, 2.7796e-10, 2.8316e-10, 2.8904e-10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(torch.Tensor(16), 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c69176f-4cfe-437e-a2d7-394c02632c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 9, 11, 10])\n",
      "torch.Size([11, 10, 9, 8])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(8,9,10,11)\n",
    "print(a.transpose(3,2).shape)\n",
    "print(a.permute(3,2,1,0).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff85e54-2bf2-4457-a72b-6be26fd01cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc6f7d2-acdf-4c64-9c5d-a3b8a4526644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(1, 1, 64, 64)\n",
    "B = torch.rand(1, 1, 64, 64)\n",
    "\n",
    "torch.cat([A, B], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7040b4a-f69b-42cc-a628-e5008770e95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 64, 32]), torch.Size([1, 1, 64, 32]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(1, 1, 64, 64)\n",
    "A1, A2 = A.chunk(2, dim=-1)\n",
    "A1.shape, A2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db556a1-fba4-4c29-9848-96783d5ab16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 128])\n",
      "torch.Size([1, 1, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "A = torch.rand(1, 1, 64, 256)\n",
    "print(F.glu(A, -1).shape)\n",
    "\n",
    "glu = torch.nn.GLU()\n",
    "print(glu(A).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ba14f8-33bc-4b02-8166-1c5f8c98b34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(2, 1, 64, 256)\n",
    "\n",
    "b, c, h, w = A.shape\n",
    "A = A.reshape(b, h, -1)\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "659ec296-0323-4c5e-8344-8e8d642ed7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2\n",
    "A = torch.rand(64, 256)\n",
    "linears = nn.Sequential(*[nn.Linear(256, 256) for i in range(n)])\n",
    "linears(A).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2edf87c-c9a2-4e01-bb41-0186cfc6d705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, 1., 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, 1., 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, 1., 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, 1., 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 1., 1.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 1.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask(dim: (int, int)) -> Tensor :\n",
    "    mask = torch.ones(dim)\n",
    "    mask = torch.triu(mask, diagonal=0)\n",
    "    mask = torch.log(mask.T)\n",
    "    return mask\n",
    "\n",
    "\n",
    "A = torch.ones(10, 10)\n",
    "m = mask(A.shape)\n",
    "A += m\n",
    "A.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdee14b9-2af0-4811-99f8-5607143a8e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([38, 64])\n",
      "K, V shape: torch.Size([64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 64])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(38, 152)\n",
    "X = torch.rand(64, 256)\n",
    "\n",
    "W_q = nn.Linear(in_features=152, out_features=64)\n",
    "W_k = nn.Linear(in_features=256, out_features=64)\n",
    "W_v = nn.Linear(in_features=256, out_features=64)\n",
    "\n",
    "Q = W_q(Y)\n",
    "K = W_k(X)\n",
    "V = W_v(X)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K, V shape: {K.shape}\")\n",
    "\n",
    "output = torch.matmul(torch.matmul(Q, K.T), V)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "10cb49c1-a66b-492b-98fc-126e5ed53cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 256)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(1, 1, 64, 256)\n",
    "b, c = A.shape[-2:]\n",
    "b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1e3f1c3-949f-4bfc-b7ba-7d36280c5e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1])\n",
    "\n",
    "torch.exp(-a/0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82782e1b-e7d9-4ddf-83ba-01b72666f588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

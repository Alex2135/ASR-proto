{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa66b6d9-683a-4354-952a-33c42d3467cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59821272-9f3f-408f-b527-992fb62a6222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[[-3.0902, -3.7390, -3.4956, -5.1916, -1.6517, -2.0307, -3.0985,\n",
      "          -3.1335, -2.5695, -4.4338, -2.9636, -2.3014, -4.7290, -2.4146,\n",
      "          -2.8782, -4.7997, -3.2969, -4.4470, -4.6960, -3.7967]],\n",
      "\n",
      "        [[-3.5079, -5.3225, -4.1056, -4.3521, -3.6322, -3.6178, -1.7566,\n",
      "          -3.5656, -2.7448, -3.7311, -3.3192, -4.1413, -3.8689, -2.2083,\n",
      "          -1.5062, -3.9838, -4.9298, -2.0877, -3.4894, -4.7769]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "Targets: tensor([[19, 10, 10, 17, 17,  9, 13, 13,  9, 12, 17, 10, 10,  6, 11,  6,  3,  8,\n",
      "         18, 15, 19,  5,  5, 14,  6,  8,  5, 16,  5, 18]])\n",
      "Input lengths:\n",
      " tensor([50])\n",
      "Target lengths:\n",
      " tensor([25])\n"
     ]
    }
   ],
   "source": [
    "T = 50\n",
    "C = 20\n",
    "N = 1\n",
    "S = 30\n",
    "\n",
    "S_min = 10\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "print(\"Inputs:\", input[:2])\n",
    "\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "print(\"\\n\\nTargets:\", target[:2])\n",
    "\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "print(\"Input lengths:\\n\", input_lengths)\n",
    "\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "print(\"Target lengths:\\n\", target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca59d1f-7e79-4d4d-a4a5-04c1e2d34385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0159,  0.0010,  0.0012,  0.0002,  0.0077,  0.0052,  0.0018,\n",
       "           0.0017,  0.0031,  0.0005,  0.0021,  0.0040,  0.0004,  0.0036,\n",
       "           0.0022,  0.0003,  0.0015,  0.0005,  0.0004, -0.0214]],\n",
       "\n",
       "        [[-0.0157,  0.0002,  0.0007,  0.0005,  0.0011,  0.0011,  0.0069,\n",
       "           0.0011,  0.0026,  0.0010, -0.0140,  0.0006,  0.0008,  0.0044,\n",
       "           0.0089,  0.0007,  0.0003,  0.0050,  0.0012, -0.0072]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "input.grad[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8be60f-96c9-4025-8dac-b9f62f91c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<blank>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, ' ': 4, 'а': 5, 'б': 6, 'в': 7, 'г': 8, 'д': 9, 'е': 10, 'ж': 11, 'з': 12, 'и': 13, 'й': 14, 'к': 15, 'л': 16, 'м': 17, 'н': 18, 'о': 19, 'п': 20, 'р': 21, 'с': 22, 'т': 23, 'у': 24, 'ф': 25, 'х': 26, 'ц': 27, 'ч': 28, 'ш': 29, 'щ': 30, 'ь': 31, 'ю': 32, 'я': 33, 'є': 34, 'і': 35, 'ї': 36, 'ґ': 37}\n",
      "{0: '<blank>', 1: '<sos>', 2: '<eos>', 3: '<unk>', 4: ' ', 5: 'а', 6: 'б', 7: 'в', 8: 'г', 9: 'д', 10: 'е', 11: 'ж', 12: 'з', 13: 'и', 14: 'й', 15: 'к', 16: 'л', 17: 'м', 18: 'н', 19: 'о', 20: 'п', 21: 'р', 22: 'с', 23: 'т', 24: 'у', 25: 'ф', 26: 'х', 27: 'ц', 28: 'ч', 29: 'ш', 30: 'щ', 31: 'ь', 32: 'ю', 33: 'я', 34: 'є', 35: 'і', 36: 'ї', 37: 'ґ'}\n"
     ]
    }
   ],
   "source": [
    "extra_tokens = [\"<blank>\", \"<sos>\", \"<eos>\", \"<unk>\", \" \"]\n",
    "tokens = extra_tokens + ['а', 'б', 'в', 'г', 'д', \n",
    "        'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', \n",
    "        'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', \n",
    "        'х', 'ц', 'ч', 'ш', 'щ', 'ь', 'ю', 'я',\n",
    "        'є', 'і', 'ї', 'ґ']\n",
    "char_to_index = {c:i for i, c in enumerate(tokens)}\n",
    "index_to_char = {i:c for i, c in enumerate(tokens)}\n",
    "print(char_to_index)\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86c7d8c-a1f3-44e9-a370-48447b0a0b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence \n",
      "'Московитам дозволено створити свою державу а татарам чеченцям – ні Але це – расизм'\n",
      "to indeces:\n",
      "\n",
      "1 17 19 22 15 19 7 13 23 5 17 4 9 19 12 7 19 16 10 18 19 4 22 23 7 19 21 13 23 13 4 22 7 19 32 4 9 10 21 11 5 7 24 4 5 4 23 5 23 5 21 5 17 4 28 10 28 10 18 27 33 17 4 18 35 4 5 16 10 4 27 10 4 21 5 22 13 12 17 2 \n",
      "\n",
      "Sentence \n",
      "'Московитам дозволено створити свою державу а татарам чеченцям – ні Але це – расизм'\n",
      "from indeces to chars:\n",
      "\n",
      "<sos> м о с к о в и т а м   д о з в о л е н о   с т в о р и т и   с в о ю   д е р ж а в у   а   т а т а р а м   ч е ч е н ц я м   н і   а л е   ц е   р а с и з м <eos> "
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_stop_signs(sentence):\n",
    "    stop_signs = string.punctuation + \"–—»«…“”’\"\n",
    "    for sign in stop_signs:\n",
    "        sentence = sentence.replace(sign, \"\")\n",
    "    return sentence\n",
    "\n",
    "def sentence_to_indeces(sentence, cti : dict):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        cti - char to index dictionary\n",
    "    \"\"\"\n",
    "    sent = remove_stop_signs(sentence)\n",
    "    sent = sent.lower()\n",
    "    sent = sent.split()\n",
    "    result = []\n",
    "    for word in sent:\n",
    "        for c in word:\n",
    "            result.append(cti.get(c, cti[\"<unk>\"]))\n",
    "        result.append(cti[\" \"])\n",
    "    result = result[:-1]\n",
    "    result = [cti[\"<sos>\"]] + result + [cti[\"<eos>\"]]\n",
    "    return result\n",
    "\n",
    "\n",
    "sent = \"Московитам дозволено створити свою державу а татарам чеченцям – ні Але це – расизм\"\n",
    "sent_to_idxs = sentence_to_indeces(sent, char_to_index)\n",
    "print(f\"Sentence \\n'{sent}'\\nto indeces:\\n\")\n",
    "for i in sent_to_idxs:\n",
    "    print(i, end=\" \")\n",
    "\n",
    "print(f\"\\n\\nSentence \\n'{sent}'\\nfrom indeces to chars:\\n\")\n",
    "for i in sent_to_idxs:\n",
    "    print(index_to_char[i], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb4b99c-d8a2-44a7-b16c-6bee91f642c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"D:\\\\ML\\\\Speech recognition\\\\NLP_diploma\\\\uk\"\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "436b932a-c5ec-4ef0-914d-965cb9b11790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max symbols 137\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "is_show_prints = False\n",
    "for sent in train_df[\"sentence\"]:\n",
    "    sent_to_idxs = sentence_to_indeces(sent, char_to_index)\n",
    "    if len(sent_to_idxs) > max:\n",
    "        max = len(sent_to_idxs)    \n",
    "    \n",
    "    if is_show_prints:\n",
    "        print(f\"Sentence \\n'{sent}'\\nto indeces:\")\n",
    "        for i in sent_to_idxs:\n",
    "            print(i, end=\" \")\n",
    "        print(f\"\\n\\nfrom indeces to chars:\")\n",
    "        for i in sent_to_idxs:\n",
    "            print(index_to_char[i], end=\" \")\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        \n",
    "print(f\"Max symbols {max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b787ba5-da22-4a74-b748-6f9f1d8ded9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 152)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41aafa1-e953-4fb1-96d9-552c9d5a7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER PARAMS\n",
    "# Encoder inputs\n",
    "enc_n_feats = 256   # spectrogram height\n",
    "enc_d_model = 1024  # spectrogram max width\n",
    "\n",
    "# DECODER PARAMS\n",
    "# Decoder inputs\n",
    "dec_n_feats = 38    # len(tokens)\n",
    "dec_d_model = 152   # maximum symbols in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a1e4bf1-1291-4d52-9d2c-131a1788b5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic: 'Московитам дозволено створити свою державу а татарам чеченцям – ні Але це – расизм'\n",
      "idxs_to_onehot result:\n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "(80, 38)\n",
      "\n",
      "onehot_matrix_to_idxs result:\n",
      "[ 1 17 19 22 15 19  7 13 23  5 17  4  9 19 12  7 19 16 10 18 19  4 22 23\n",
      "  7 19 21 13 23 13  4 22  7 19 32  4  9 10 21 11  5  7 24  4  5  4 23  5\n",
      " 23  5 21  5 17  4 28 10 28 10 18 27 33 17  4 18 35  4  5 16 10  4 27 10\n",
      "  4 21  5 22 13 12 17  2]\n",
      "<sos> м о с к о в и т а м   д о з в о л е н о   с т в о р и т и   с в о ю   д е р ж а в у   а   т а т а р а м   ч е ч е н ц я м   н і   а л е   ц е   р а с и з м <eos> "
     ]
    }
   ],
   "source": [
    "def idxs_to_onehot(sent_idxs, length:int):\n",
    "    result = []\n",
    "    for number in sent_idxs:\n",
    "        onehot = np.zeros((length,))\n",
    "        onehot[number] = 1\n",
    "        result.append(onehot)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def onehot_matrix_to_idxs(onehots):\n",
    "    result = []\n",
    "    for i in range(onehots.shape[0]):\n",
    "        onehot = onehots[i, :]\n",
    "        number = np.argmax(onehot)\n",
    "        result.append(number)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "sent = \"Московитам дозволено створити свою державу а татарам чеченцям – ні Але це – расизм\"\n",
    "print(f\"Basic: '{sent}'\")\n",
    "\n",
    "sent_to_idxs = sentence_to_indeces(sent, char_to_index)\n",
    "a = idxs_to_onehot(sent_to_idxs, dec_n_feats)\n",
    "print(\"idxs_to_onehot result:\")\n",
    "print(a)\n",
    "print(a.shape)\n",
    "\n",
    "print(\"\\nonehot_matrix_to_idxs result:\")\n",
    "r = onehot_matrix_to_idxs(a)\n",
    "print(r)\n",
    "for i in r:\n",
    "    print(index_to_char[i], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4cc699f-bba9-494a-b2be-e851745245a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "A = torch.Tensor([1, 2, 3]).long()\n",
    "print(A)\n",
    "\n",
    "F.one_hot(A, num_classes=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "695d73ec-f2ae-4ef3-8173-aea32cb55127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 38]) torch.Size([80, 38])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3040"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_num = sentence_to_indeces(sent, char_to_index) # list of ints\n",
    "A = torch.Tensor(sent_num).long()\n",
    "a1 = F.one_hot(A, num_classes=dec_n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "179e2440-f5bb-4274-bddd-900eb4dbf1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 38]) tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0]])\n",
      "[tensor(1), tensor(17), tensor(19), tensor(22), tensor(15), tensor(19), tensor(7), tensor(13), tensor(23), tensor(5), tensor(17), tensor(4), tensor(9), tensor(19), tensor(12), tensor(7), tensor(19), tensor(16), tensor(10), tensor(18), tensor(19), tensor(4), tensor(22), tensor(23), tensor(7), tensor(19), tensor(21), tensor(13), tensor(23), tensor(13), tensor(4), tensor(22), tensor(7), tensor(19), tensor(32), tensor(4), tensor(9), tensor(10), tensor(21), tensor(11), tensor(5), tensor(7), tensor(24), tensor(4), tensor(5), tensor(4), tensor(23), tensor(5), tensor(23), tensor(5), tensor(21), tensor(5), tensor(17), tensor(4), tensor(28), tensor(10), tensor(28), tensor(10), tensor(18), tensor(27), tensor(33), tensor(17), tensor(4), tensor(18), tensor(35), tensor(4), tensor(5), tensor(16), tensor(10), tensor(4), tensor(27), tensor(10), tensor(4), tensor(21), tensor(5), tensor(22), tensor(13), tensor(12), tensor(17), tensor(2)]\n",
      "<sos>московитам дозволено створити свою державу а татарам чеченцям ні але це расизм<eos>\n"
     ]
    }
   ],
   "source": [
    "class LangHandling():\n",
    "    def __init__(self, tokens):\n",
    "        self.token_to_index = {c:i for i, c in enumerate(tokens)}\n",
    "        self.index_to_token = {i:c for i, c in enumerate(tokens)}\n",
    "        self.num_classes = len(tokens)\n",
    "\n",
    "\n",
    "class LangCharHandling(LangHandling):\n",
    "    def __init__(self, tokens):\n",
    "        super().__init__(tokens)\n",
    "        self.stop_signs = string.punctuation + \"–—»«…“”’\"\n",
    "        \n",
    "    def remove_stop_signs(self, sentence):\n",
    "        for sign in self.stop_signs:\n",
    "            sentence = sentence.replace(sign, \"\")\n",
    "        return sentence\n",
    "\n",
    "    def sentence_to_indeces(self, sentence):\n",
    "        sent = self.remove_stop_signs(sentence)\n",
    "        sent = sent.lower()\n",
    "        sent = sent.split()\n",
    "        result = []\n",
    "        for word in sent:\n",
    "            for c in word:\n",
    "                char = self.token_to_index.get(c, self.token_to_index[\"<unk>\"])\n",
    "                result.append(char)\n",
    "            result.append(self.token_to_index[\" \"])\n",
    "        result = result[:-1]\n",
    "        result = [self.token_to_index[\"<sos>\"]] + result + [self.token_to_index[\"<eos>\"]]\n",
    "        return result\n",
    "\n",
    "    def sentence_to_one_hots(self, sent):\n",
    "        sent_to_idxs = self.sentence_to_indeces(sent)\n",
    "        sent_to_idxs = torch.Tensor(sent_to_idxs).long()\n",
    "        one_hots = F.one_hot(sent_to_idxs, num_classes=self.num_classes)\n",
    "        return one_hots\n",
    "    \n",
    "    def one_hots_to_sentence(self, one_hots):\n",
    "        result = \"\"\n",
    "        idxs = self.onehot_matrix_to_idxs(one_hots)\n",
    "        print(idxs)\n",
    "        for index in idxs:\n",
    "            result += self.index_to_token[int(index)]\n",
    "        return result\n",
    "        \n",
    "    def onehot_matrix_to_idxs(self, one_hots):\n",
    "        result = []\n",
    "        for i in range(one_hots.shape[0]):\n",
    "            one_hot = one_hots[i, :]\n",
    "            number = np.argmax(one_hot)\n",
    "            result.append(number)\n",
    "        return result\n",
    "\n",
    "    \n",
    "sent = \"Московитам дозволено створити свою державу а татарам чеченцям – ні Але це – расизм\"\n",
    "lang_handle = LangCharHandling(tokens)\n",
    "one_hots = lang_handle.sentence_to_one_hots(sent)\n",
    "print(one_hots.shape, one_hots)\n",
    "sent_result = lang_handle.one_hots_to_sentence(one_hots)\n",
    "print(sent_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a46a0-8077-4adc-87d5-66ad2fb74a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
